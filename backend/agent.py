#!/usr/bin/env python3
"""
Daily AI Insight - AI ë‰´ìŠ¤ íë ˆì´ì…˜ ì—ì´ì „íŠ¸
ë§¤ì¼ ì£¼ìš” AI RSS í”¼ë“œë¥¼ ìˆ˜ì§‘í•˜ê³  GPT-4o-minië¡œ ë¶„ì„í•˜ì—¬ ìƒìœ„ 10ê°œë¥¼ ì„ ì •í•©ë‹ˆë‹¤.
"""

import os
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dotenv import load_dotenv
import feedparser
import requests
from openai import OpenAI
from supabase import create_client, Client
from dateutil import parser as date_parser

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
openai_client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
supabase: Client = create_client(
    os.getenv('SUPABASE_URL'),
    os.getenv('SUPABASE_KEY')
)

# RSS í”¼ë“œ ì†ŒìŠ¤ (AI ê´€ë ¨ ì£¼ìš” ë§¤ì²´)
RSS_FEEDS = [
    {
        'name': 'TechCrunch AI',
        'url': 'https://techcrunch.com/category/artificial-intelligence/feed/'
    },
    {
        'name': 'OpenAI Blog',
        'url': 'https://openai.com/blog/rss.xml'
    },
    {
        'name': 'MIT Technology Review AI',
        'url': 'https://www.technologyreview.com/topic/artificial-intelligence/feed'
    },
    {
        'name': 'Ars Technica AI',
        'url': 'https://feeds.arstechnica.com/arstechnica/technology-lab'
    },
    {
        'name': 'AI News',
        'url': 'https://www.artificialintelligence-news.com/feed/'
    },
    {
        'name': 'VentureBeat â€“ AI Section',
        'url': 'https://venturebeat.com/category/ai/feed/'
    },
    {
        'name': 'Google AI Blog',
        'url': 'https://blog.google/technology/ai/rss/'
    },
    {
        'name': 'Artificial Intelligence (cs.AI)',
        'url': 'https://export.arxiv.org/rss/cs.AI'
    }
]


def fetch_rss_feeds(hours_ago: int = 24) -> List[Dict]:
    """
    RSS í”¼ë“œì—ì„œ ìµœê·¼ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    
    Args:
        hours_ago: ìµœê·¼ ëª‡ ì‹œê°„ ë‚´ì˜ ê¸°ì‚¬ë¥¼ ê°€ì ¸ì˜¬ì§€ (ê¸°ë³¸ê°’: 24ì‹œê°„)
    
    Returns:
        ê¸°ì‚¬ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸
    """
    cutoff_time = datetime.now() - timedelta(hours=hours_ago)
    all_articles = []
    
    logger.info(f"Fetching RSS feeds from {len(RSS_FEEDS)} sources...")
    
    for feed_info in RSS_FEEDS:
        try:
            logger.info(f"Parsing {feed_info['name']}...")
            feed = feedparser.parse(feed_info['url'])
            
            for entry in feed.entries:
                try:
                    # ë‚ ì§œ íŒŒì‹± (ì—¬ëŸ¬ í˜•ì‹ ì‹œë„)
                    published_date = None
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_date = datetime(*entry.published_parsed[:6])
                    elif hasattr(entry, 'published'):
                        published_date = date_parser.parse(entry.published)
                    elif hasattr(entry, 'updated_parsed') and entry.updated_parsed:
                        published_date = datetime(*entry.updated_parsed[:6])
                    
                    # ìµœê·¼ 24ì‹œê°„ ë‚´ ê¸°ì‚¬ë§Œ í•„í„°ë§
                    if published_date and published_date >= cutoff_time:
                        # ë³¸ë¬¸ ì¶”ì¶œ (summary ë˜ëŠ” content ì‚¬ìš©)
                        content = ''
                        if hasattr(entry, 'summary'):
                            content = entry.summary
                        elif hasattr(entry, 'content'):
                            content = entry.content[0].value if entry.content else ''
                        
                        article = {
                            'title': entry.title,
                            'url': entry.link,
                            'content': content[:2000],  # ìµœëŒ€ 2000ìë¡œ ì œí•œ
                            'published_date': published_date,
                            'source': feed_info['name']
                        }
                        all_articles.append(article)
                        logger.info(f"  âœ“ {entry.title[:50]}...")
                        
                except Exception as e:
                    logger.warning(f"  âœ— Error parsing entry: {e}")
                    continue
                    
        except Exception as e:
            logger.error(f"Error fetching {feed_info['name']}: {e}")
            continue
    
    logger.info(f"Total articles collected: {len(all_articles)}")
    return all_articles


def analyze_with_gpt(article: Dict) -> Optional[Dict]:
    """
    GPT-4o-minië¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ì‚¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        article: ê¸°ì‚¬ ì •ë³´ ë”•ì…”ë„ˆë¦¬
    
    Returns:
        ë¶„ì„ ê²°ê³¼ (ìš”ì•½, íƒœê·¸, ì ìˆ˜) ë˜ëŠ” None
    """
    system_prompt = """ë„ˆëŠ” IT ì „ë¬¸ ì—ë””í„°ì´ì AI íŠ¸ë Œë“œ ë¶„ì„ê°€ë‹¤.
ì£¼ì–´ì§„ AI ê´€ë ¨ ê¸°ì‚¬ë¥¼ ì½ê³  ë‹¤ìŒ ì •ë³´ë¥¼ JSON í˜•ì‹ìœ¼ë¡œ ì¶œë ¥í•˜ë¼:

1. title: ê¸°ì‚¬ ì œëª©ì„ ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ë¡œ ë²ˆì—­ (ì›ì œì˜ ì˜ë¯¸ë¥¼ ìµœëŒ€í•œ ì‚´ë¦¼)
2. summary: í•œêµ­ì–´ë¡œ ì‘ì„±ëœ 3ì¤„ ìš”ì•½ (ê° ì¤„ì€ ì™„ì „í•œ ë¬¸ì¥, ë°°ì—´ í˜•íƒœ)
3. tags: í•µì‹¬ í‚¤ì›Œë“œ 3ê°œ (ì˜ì–´ ë˜ëŠ” í•œêµ­ì–´, ë°°ì—´ í˜•íƒœ)
4. importance_score: 1~10ì  (AI ì—…ê³„ ì˜í–¥ë„ ê¸°ì¤€)

í‰ê°€ ê¸°ì¤€:
- ê¸°ìˆ  í˜ì‹ ì„±: ìƒˆë¡œìš´ ëª¨ë¸, ì•Œê³ ë¦¬ì¦˜, ì„œë¹„ìŠ¤ ì¶œì‹œ
- ì‚°ì—… ì˜í–¥ë„: ì£¼ìš” ê¸°ì—… ë™í–¥, ì‹œì¥ ë³€í™”
- ì‚¬íšŒì  íŒŒê¸‰ë ¥: ìœ¤ë¦¬, ê·œì œ, ê´‘ë²”ìœ„í•œ ì˜í–¥

JSON í˜•ì‹:
{
  "title": "í•œêµ­ì–´ë¡œ ë²ˆì—­ëœ ì œëª©",
  "summary": ["ìš”ì•½ ë¬¸ì¥ 1", "ìš”ì•½ ë¬¸ì¥ 2", "ìš”ì•½ ë¬¸ì¥ 3"],
  "tags": ["íƒœê·¸1", "íƒœê·¸2", "íƒœê·¸3"],
  "importance_score": 8
}

9-10ì : ì—…ê³„ íŒë„ë¥¼ ë°”ê¿€ ì´ˆëŒ€í˜• ë‰´ìŠ¤ (ì˜ˆ: ChatGPT ì¶œì‹œê¸‰)
7-8ì : ì£¼ìš” ê¸°ì—…ì˜ ì¤‘ìš”í•œ ë°œí‘œ, ê·œì œ ë³€í™”
5-6ì : ì£¼ëª©í•  ë§Œí•œ ê¸°ìˆ  ê°œì„ , ì„œë¹„ìŠ¤ ì—…ë°ì´íŠ¸
3-4ì : ì†Œì†Œí•œ ì—…ë°ì´íŠ¸, ë³´ë„ìë£Œì„± ë‰´ìŠ¤
1-2ì : ë¯¸ë¯¸í•œ ì˜í–¥
ğŸ¯ ì‹¤ì œ ì‘ë™ ë°©ì‹
GPTê°€ ê¸°ì‚¬ ë¶„ì„: ì œëª©, ë³¸ë¬¸, ì¶œì²˜ë¥¼ ì¢…í•©ì ìœ¼ë¡œ í‰ê°€
3ê°€ì§€ ê¸°ì¤€ ì ìš©: í˜ì‹ ì„± + ì˜í–¥ë„ + íŒŒê¸‰ë ¥ì„ ì¢…í•©
1-10ì  ìŠ¤ì½”ì–´ë§: ê°ê´€ì  ê¸°ì¤€ì— ë”°ë¼ ì ìˆ˜ ë¶€ì—¬
ìƒìœ„ 10ê°œ ì„ ì •: ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
"""
    
    user_prompt = f"""ê¸°ì‚¬ ì œëª©: {article['title']}

ê¸°ì‚¬ ë‚´ìš©:
{article['content']}

ì¶œì²˜: {article['source']}"""
    
    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0.3,
            max_tokens=500
        )
        
        analysis = json.loads(response.choices[0].message.content)
        
        # ìœ íš¨ì„± ê²€ì¦
        if not all(key in analysis for key in ['title', 'summary', 'tags', 'importance_score']):
            logger.warning(f"Invalid analysis format for {article['title']}")
            return None
        
        if len(analysis['summary']) != 3:
            logger.warning(f"Summary should have exactly 3 items for {article['title']}")
            return None
        
        if not (1 <= analysis['importance_score'] <= 10):
            logger.warning(f"Invalid importance_score for {article['title']}")
            return None
        
        logger.info(f"  Analyzed: {article['title'][:50]}... (Score: {analysis['importance_score']})")
        return analysis
        
    except Exception as e:
        logger.error(f"Error analyzing article {article['title']}: {e}")
        return None


def select_top_articles(articles: List[Dict], limit: int = 5) -> List[Dict]:
    """
    GPT ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒìœ„ ê¸°ì‚¬ë¥¼ ì„ ì •í•©ë‹ˆë‹¤.
    
    Args:
        articles: ë¶„ì„ëœ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
        limit: ì„ ì •í•  ê¸°ì‚¬ ìˆ˜
    
    Returns:
        ìƒìœ„ ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    """
    logger.info(f"Analyzing {len(articles)} articles with GPT-4o-mini...")
    
    analyzed_articles = []
    
    for article in articles:
        analysis = analyze_with_gpt(article)
        if analysis:
            # ì›ë³¸ ì˜ì–´ ì œëª© ë³´ì¡´
            original_title = article['title']
            analyzed_articles.append({
                **article,
                **analysis,
                'original_title': original_title  # ì›ë³¸ ì œëª© ë³´ì¡´
            })
    
    # ì¤‘ìš”ë„ ì ìˆ˜ë¡œ ì •ë ¬í•˜ì—¬ ìƒìœ„ ì„ ì •
    top_articles = sorted(
        analyzed_articles,
        key=lambda x: x['importance_score'],
        reverse=True
    )[:limit]
    
    logger.info(f"Selected top {len(top_articles)} articles")
    return top_articles


def save_to_supabase(articles: List[Dict]) -> int:
    """
    ì„ ì •ëœ ê¸°ì‚¬ë¥¼ Supabaseì— ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        articles: ì €ì¥í•  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸
    
    Returns:
        ì €ì¥ëœ ê¸°ì‚¬ ìˆ˜
    """
    logger.info(f"Saving {len(articles)} articles to Supabase...")
    
    saved_count = 0
    
    for article in articles:
        try:
            # ë°ì´í„° ì¤€ë¹„
            data = {
                'title': article.get('title', article.get('original_title', 'Untitled')),  # GPTê°€ ë²ˆì—­í•œ í•œêµ­ì–´ ì œëª© ì‚¬ìš©
                'summary': article['summary'],  # JSONB ë°°ì—´
                'tags': article['tags'],  # TEXT[] ë°°ì—´
                'original_url': article['url'],
                'importance_score': article['importance_score'],
                'published_at': article['published_date'].strftime('%Y-%m-%d')
            }
            
            # Upsert (URL ê¸°ì¤€ ì¤‘ë³µ ë°©ì§€)
            result = supabase.table('news_items').upsert(
                data,
                on_conflict='original_url'
            ).execute()
            
            saved_count += 1
            logger.info(f"  âœ“ Saved: {article['title'][:50]}...")
            
        except Exception as e:
            logger.error(f"  âœ— Error saving article {article['title']}: {e}")
            continue
    
    logger.info(f"Successfully saved {saved_count}/{len(articles)} articles")
    return saved_count


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("=" * 80)
    logger.info("Daily AI Insight - News Curation Agent")
    logger.info("=" * 80)
    
    try:
        # 1. RSS í”¼ë“œ ìˆ˜ì§‘
        articles = fetch_rss_feeds(hours_ago=24)
        
        if not articles:
            logger.warning("No articles found in the last 24 hours")
            return
        
        # 2. GPT ë¶„ì„ ë° ìƒìœ„ 10ê°œ ì„ ì •
        top_articles = select_top_articles(articles, limit=10)
        
        if not top_articles:
            logger.warning("No articles passed the analysis")
            return
        
        # 3. Supabaseì— ì €ì¥
        saved_count = save_to_supabase(top_articles)
        
        logger.info("=" * 80)
        logger.info(f"âœ… Curation complete! {saved_count} articles saved.")
        logger.info("=" * 80)
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        for i, article in enumerate(top_articles, 1):
            logger.info(f"{i}. {article['title']}")
            logger.info(f"   Score: {article['importance_score']}, Tags: {', '.join(article['tags'])}")
        
    except Exception as e:
        logger.error(f"Fatal error: {e}")
        raise


if __name__ == '__main__':
    main()
